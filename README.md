# ğŸš€ LLM Stream API

> **Proxy OpenAI scalable** avec Celery + Redis pour gÃ©rer la charge des appels LLM.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Celery](https://img.shields.io/badge/Celery-5.4-green.svg)](https://docs.celeryq.dev)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)
[![OpenAI Compatible](https://img.shields.io/badge/OpenAI-compatible-orange.svg)](https://platform.openai.com)

---

## ğŸ¯ ProblÃ¨me rÃ©solu

Les appels LLM (OpenAI) prennent **10-60 secondes** et bloquent vos workers HTTP.

**Ce proxy** :
- âš¡ File d'attente intelligente (Celery)
- ğŸ”„ Rate limiting centralisÃ©
- ğŸ“¡ Streaming SSE temps rÃ©el
- ğŸ”Œ **Compatible SDK OpenAI** (drop-in replacement)

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SDK OpenAI     â”‚â”€â”€â”€â”€â–¶â”‚   PROXY API    â”‚â”€â”€â”€â”€â–¶â”‚   Celery/Redis  â”‚
â”‚  (any language) â”‚     â”‚  /v1/chat/...   â”‚     â”‚   (queue)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                                                  â”‚   OpenAI    â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”Œ Usage (SDK OpenAI)

### Python

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8007/v1",  # Proxy
    api_key="not-needed"
)

# Streaming
stream = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)
for chunk in stream:
    print(chunk.choices[0].delta.content or "", end="")

# Non-streaming
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=False
)
print(response.choices[0].message.content)
```

### Node.js

```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'http://localhost:8007/v1',
  apiKey: 'not-needed'
});

const stream = await client.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ role: 'user', content: 'Hello!' }],
  stream: true
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}
```

### cURL

```bash
curl http://localhost:8007/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "gpt-4o-mini", "messages": [{"role": "user", "content": "Hello!"}]}'
```

---

## ğŸ“ Structure

```
llm_fastapi_mq/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI
â”‚   â”‚   â””â”€â”€ proxy.py          # Proxy OpenAI compatible
â”‚   â”œâ”€â”€ tasks/llm_tasks.py    # TÃ¢ches Celery
â”‚   â”œâ”€â”€ celery_app.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.worker
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ui/                       # Interface chat
â”‚   â”œâ”€â”€ chat.html
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ client_openai.py      # Exemple Python
â”‚   â””â”€â”€ client_openai.js      # Exemple Node.js
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ run.sh
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration `.env`

```env
# === REQUIS ===
OPENAI_API_KEY=sk-proj-xxxxx

# === REDIS ===
REDIS_URL=redis://redis:6379/0

# === API ===
PORT=8007
UVICORN_WORKERS=4

# === CELERY (gevent pool) ===
CELERY_POOL=gevent
CELERY_CONCURRENCY=100
CELERY_QUEUES=high,default,low
CELERY_LOGLEVEL=info

# === UI ===
UI_PORT=3000
API_URL=http://localhost:8007

# === MONITORING ===
FLOWER_PORT=5555

# === RATE LIMITING (selon tier OpenAI) ===
CELERY_RATE_LIMIT=500/m
```

---

## ğŸš€ DÃ©marrage

```bash
# 1. Config
cp .env.example .env

# 2. Lancer
./run.sh start

# 3. Tester
python examples/client_openai.py

# 4. Ouvrir
#    UI:   http://localhost:3000
#    API:  http://localhost:8007/docs
```

---

## ğŸ“¡ Endpoints

### Proxy OpenAI (`/v1`)

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `POST` | `/v1/chat/completions` | Chat (streaming & non-streaming) |
| `GET` | `/v1/models` | Liste des modÃ¨les |

### API interne

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `POST` | `/chat` | Chat async (Celery) |
| `GET` | `/stream/{session_id}` | Stream SSE |
| `POST` | `/embeddings` | Batch embeddings |
| `GET` | `/stats` | Stats queues |

---

## ğŸ³ Services Docker

| Service | Port | Description |
|---------|------|-------------|
| `api` | 8007 | FastAPI + Proxy OpenAI |
| `worker` | - | Celery workers |
| `redis` | - | Broker (interne) |
| `ui` | 3000 | Interface chat |
| `flower` | 5555 | Monitoring (optionnel) |

---

## ğŸ› ï¸ Commandes

```bash
./run.sh start         # DÃ©marre tout
./run.sh stop          # ArrÃªte
./run.sh restart       # RedÃ©marre
./run.sh logs          # Tous les logs
./run.sh logs api      # Logs API
./run.sh logs worker   # Logs Worker
./run.sh status        # Status
./run.sh scale 5       # 5 workers
./run.sh monitoring    # + Flower
./run.sh test          # Tests
./run.sh build         # Build
./run.sh clean         # Nettoie
```

---

## ğŸ“Š Scaling

```
Workers = (RequÃªtes/min) Ã— (Temps moyen/min) / Concurrency

Exemple: 100 req/min Ã— 0.5 min / 4 = 13 workers
```

| Charge | Workers | Concurrency |
|--------|---------|-------------|
| Dev | 1 | 2 |
| 50 req/min | 4 | 4 |
| 200 req/min | 10 | 4 |
| 500 req/min | 25 | 4 |

---

## ğŸ”§ Features

- âœ… **Proxy OpenAI compatible** (SDK standard)
- âœ… File d'attente Celery
- âœ… Rate limiting (token bucket Redis)
- âœ… Retry automatique (backoff exponentiel)
- âœ… 3 queues prioritaires (high/default/low)
- âœ… Streaming SSE
- âœ… Monitoring Flower
- âœ… Multi-langage (Python, Node, Go, etc.)

---

## ğŸ“„ License

MIT
