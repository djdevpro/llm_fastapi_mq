# ğŸš€ LLM Stream API

> **API LLM scalable** avec Celery pour gÃ©rer les requÃªtes OpenAI en parallÃ¨le.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Celery](https://img.shields.io/badge/Celery-5.4-green.svg)](https://docs.celeryq.dev)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)
[![Tests](https://img.shields.io/badge/tests-passing-success.svg)](tests/)

---

## ğŸ¯ ProblÃ¨me rÃ©solu

Les appels LLM (OpenAI) prennent **10-60 secondes** et bloquent vos workers HTTP.

**Cette architecture** :
- âš¡ Retourne **immÃ©diatement** (~100ms)
- ğŸ”„ Traite en **parallÃ¨le** via Celery
- ğŸ“¡ Streame via **SSE** (Server-Sent Events)

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      POST /chat       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (UI:3000)  â”‚ â—„â”€â”€ task_id + session â”‚   (API:8007)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚ SSE                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚     Celery      â”‚
       â”‚                              â”‚    Workers      â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚ GET /stream/{session}        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     Redis       â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Structure

```
llm_fastapi_mq/
â”œâ”€â”€ app/                      # Code source
â”‚   â”œâ”€â”€ api/main.py           # FastAPI
â”‚   â”œâ”€â”€ tasks/llm_tasks.py    # TÃ¢ches Celery
â”‚   â”œâ”€â”€ celery_app.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ docker/                   # Docker
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.worker
â”‚   â”œâ”€â”€ entrypoint-api.sh
â”‚   â”œâ”€â”€ entrypoint-worker.sh
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ ui/                       # Interface web
â”‚   â”œâ”€â”€ chat.html
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_celery.py
â”‚
â”œâ”€â”€ run.sh
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration `.env`

```env
# === REQUIS ===
OPENAI_API_KEY=sk-proj-xxxxx

# === BROKER ===
BROKER=redis
REDIS_URL=redis://redis:6379/0

# Pour RabbitMQ (optionnel)
# BROKER=rabbitmq
# RABBITMQ_URL=amqps://user:pass@host/vhost

# === API ===
PORT=8007
UVICORN_WORKERS=4

# === CELERY ===
CELERY_CONCURRENCY=4
CELERY_QUEUES=high,default,low
CELERY_LOGLEVEL=info

# === MONITORING ===
FLOWER_PORT=5555

# === UI ===
WEB_PORT=3000

# === RATE LIMITING ===
LLM_RPM=500
LLM_TPM=100000
```

---

## ğŸš€ DÃ©marrage

```bash
# 1. Config
cp .env.example .env

# 2. Lancer
./run.sh start

# 3. Ouvrir
#    UI:  http://localhost:3000
#    API: http://localhost:8007
#    Docs: http://localhost:8007/docs
```

---

## ğŸ“¡ Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/health/full` | Status complet |
| `POST` | `/chat` | **Chat async (Celery)** âš¡ |
| `GET` | `/chat/{task_id}` | Status tÃ¢che |
| `GET` | `/stream/{session_id}` | Stream SSE |
| `POST` | `/embeddings` | Batch embeddings |
| `GET` | `/stats` | Stats queues |

### Exemple

```bash
# 1. Envoie (retour ~100ms)
curl -X POST http://localhost:8007/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!", "priority": 5}'

# RÃ©ponse:
{
  "status": "queued",
  "task_id": "xxx",
  "session_id": "yyy",
  "stream_url": "/stream/yyy"
}

# 2. Stream SSE
curl -N http://localhost:8007/stream/yyy
```

---

## ğŸ³ Services Docker

| Service | Port | Description |
|---------|------|-------------|
| `ui` | 3000 | Interface chat |
| `api` | 8007 | FastAPI |
| `worker` | - | Celery workers |
| `redis` | - | Broker (interne) |
| `flower` | 5555 | Monitoring (optionnel) |

---

## ğŸ› ï¸ Commandes

```bash
./run.sh start         # DÃ©marre tout
./run.sh stop          # ArrÃªte
./run.sh restart       # RedÃ©marre
./run.sh logs          # Tous les logs
./run.sh logs api      # Logs API
./run.sh logs worker   # Logs Worker
./run.sh status        # Status
./run.sh scale 5       # 5 workers
./run.sh monitoring    # + Flower
./run.sh test          # Tests
./run.sh build         # Build
./run.sh clean         # Nettoie
```

---

## ğŸ“Š Scaling

| Charge | Workers | Concurrency |
|--------|---------|-------------|
| Dev | 1 | 2 |
| Petit | 2 | 4 |
| Moyen | 4 | 4 |
| Prod | 8+ | 4 |

### PrioritÃ©s

```python
{"priority": 10}   # â†’ queue "high"
{"priority": 0}    # â†’ queue "default"  
{"priority": -10}  # â†’ queue "low"
```

---

## ğŸ§ª Tests

```bash
pytest tests/test_celery.py -v -s
```

---

## ğŸ”§ Features Celery

- âœ… Rate limiting (token bucket Redis)
- âœ… Retry automatique (backoff exponentiel)
- âœ… 3 queues prioritaires
- âœ… Timeout 5 min
- âœ… Monitoring Flower

---

## ğŸ“„ License

MIT
