# ğŸš€ LLM Stream + RabbitMQ

POC de streaming LLM **haute performance** avec dÃ©couplage via RabbitMQ. GÃ¨re des centaines d'utilisateurs simultanÃ©s grÃ¢ce Ã  une architecture distribuÃ©e.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)

## âœ¨ FonctionnalitÃ©s

- ğŸš€ **Streaming LLM** via OpenAI API (gpt-4o-mini)
- ğŸ“¡ **RabbitMQ** pour le dÃ©couplage producteur/consommateur
- ğŸ”„ **SSE** (Server-Sent Events) pour le streaming temps rÃ©el
- ğŸ³ **Docker** avec auto-scaling des workers
- âš¡ **Mode async** : traite 100+ requÃªtes simultanÃ©es
- ğŸ§ª **Tests pytest** inclus pour valider le parallÃ©lisme

---

## ğŸ“ Architecture

### Mode Synchrone (`/chat`) - CompatibilitÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /chat      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (Browser)  â”‚ â—„â”€â”€â”€â”€streamâ”€â”€â”€â”€â”€â”€â”€â”€ â”‚    (traite)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mode Asynchrone (`/chat/async`) - Haute charge âš¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /chat/async   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚  (Browser)  â”‚ â—„â”€â”€{session_id}â”€â”€â”€â”€â”€â”€ â”‚  (fire & forget)â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      (~50ms)          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚ Publie tÃ¢che
       â”‚ SSE                                   â–¼
       â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚    RabbitMQ     â”‚
       â”‚                              â”‚   (llm_tasks)   â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚                                       â”‚ Consomme
       â”‚                                       â–¼
       â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚  LLM Worker(s)  â”‚ Ã—N instances
       â”‚                              â”‚  (llm_worker.py)â”‚
       â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                       â”‚
       â”‚ GET /stream/{session_id}              â”‚ Publie chunks
       â”‚                                       â–¼
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚ llm_session_{id}â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Comparaison des modes

| Aspect | Sync (`/chat`) | Async (`/chat/async`) |
|--------|----------------|----------------------|
| Latence HTTP | BloquÃ© pendant gÃ©nÃ©ration | **~50ms** retour immÃ©diat |
| Workers HTTP | 1 par requÃªte active | LibÃ©rÃ© instantanÃ©ment |
| ScalabilitÃ© | LimitÃ©e par uvicorn | Workers indÃ©pendants |
| Charge max | ~50 req/s | **1000+ req/s** |
| Use case | Dev, tests | **Production** |

---

## âš™ï¸ Variables d'environnement

| Variable | Description | DÃ©faut | Requis |
|----------|-------------|--------|--------|
| `OPENAI_API_KEY` | ClÃ© API OpenAI | - | âœ… |
| `RABBIT_MQ` | URL de connexion RabbitMQ | - | âœ… |
| `UVICORN_WORKERS` | Nombre de workers HTTP (uvicorn) | `4` | âŒ |
| `LLM_WORKERS` | Nombre de workers LLM (traitement OpenAI) | `3` | âŒ |
| `PORT` | Port de l'API | `8007` | âŒ |

### Exemple `.env`

```env
# Requis
OPENAI_API_KEY=sk-your-openai-api-key
RABBIT_MQ=amqps://user:password@host/vhost

# Optionnel (scaling)
UVICORN_WORKERS=4
LLM_WORKERS=5
PORT=8007
```

---

## ğŸš€ DÃ©marrage rapide

### 1. Configuration

```bash
cp .env.example .env
# Ã‰diter .env avec vos clÃ©s
```

### 2. Build & Run

```bash
# Avec le script
./run.sh start

# Ou manuellement
docker build -t llm-fastapi-mq .
docker run -d --name llm-mq-poc \
  -p 8007:8007 \
  -e UVICORN_WORKERS=4 \
  -e LLM_WORKERS=5 \
  --env-file .env \
  llm-fastapi-mq
```

### 3. VÃ©rification

```bash
# Health check complet
curl http://localhost:8007/health/full
# {"status":"ok","rabbitmq":"connected","openai":"configured"}

# Voir les logs de dÃ©marrage
docker logs llm-mq-poc
```

---

## ğŸ“¡ API Endpoints

| MÃ©thode | Endpoint | Description | Mode |
|---------|----------|-------------|------|
| `GET` | `/health` | Health check basique | - |
| `GET` | `/health/full` | Health check + statut RabbitMQ/OpenAI | - |
| `GET` | `/test` | Test connexion OpenAI | - |
| `GET` | `/stats` | TÃ¢ches en attente dans la queue | - |
| `POST` | `/chat` | Streaming synchrone (legacy) | Sync |
| `POST` | `/chat/async` | Fire-and-forget, retourne session_id | **Async** âš¡ |
| `GET` | `/stream/{session_id}` | SSE - consomme les chunks | Async |

### Exemple : Mode Async (recommandÃ©)

```bash
# 1. Envoie la requÃªte (retour immÃ©diat ~50ms)
curl -X POST http://localhost:8007/chat/async \
  -H "Content-Type: application/json" \
  -d '{"message": "Explique-moi Docker"}'

# RÃ©ponse :
# {"status":"queued","session_id":"abc-123","stream_url":"/stream/abc-123"}

# 2. Ã‰coute le stream SSE
curl -N http://localhost:8007/stream/abc-123
```

### Exemple : Mode Sync (compatibilitÃ©)

```bash
curl -N -X POST http://localhost:8007/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Bonjour !"}'
```

---

## ğŸ§ª Tests

Tests pytest inclus pour valider le parallÃ©lisme :

```bash
# Installation des dÃ©pendances de test
pip install pytest pytest-asyncio httpx

# Lancer tous les tests
pytest tests/ -v -s

# Test spÃ©cifique : 5 requÃªtes parallÃ¨les
pytest tests/test_concurrent.py -v -s -k "test_parallel_5"

# Test de comparaison sync vs async
pytest tests/test_concurrent.py -v -s -k "test_compare"
```

### Exemple de sortie

```
==================================================
  TEST: 5 requÃªtes en parallÃ¨le
==================================================
  RequÃªte #1: 3.21s | Queue: 45ms | RÃ©ponse: 1, 2, 3...
  RequÃªte #2: 3.18s | Queue: 42ms | RÃ©ponse: A, B, C...
  RequÃªte #3: 3.25s | Queue: 48ms | RÃ©ponse: ...
  RequÃªte #4: 3.19s | Queue: 44ms | RÃ©ponse: ...
  RequÃªte #5: 3.22s | Queue: 46ms | RÃ©ponse: ...

==================================================
  RÃ‰SULTATS
==================================================
  Temps total:        3.45s
  Temps moyen/req:    3.21s
  Si sÃ©quentiel:      16.05s
  Gain parallÃ©lisme:  4.7x
==================================================

âœ“ ParallÃ©lisme confirmÃ©: 4.7x plus rapide!
```

---

## ğŸ“Š Scaling

### Calcul du nombre de workers LLM

```
workers = (requÃªtes/minute) Ã— (temps moyen gÃ©nÃ©ration en minutes)

Exemple :
- 100 requÃªtes/minute
- 30 secondes par gÃ©nÃ©ration (0.5 min)
- Workers nÃ©cessaires = 100 Ã— 0.5 = 50 workers
```

### Configuration recommandÃ©e

| Charge | UVICORN_WORKERS | LLM_WORKERS | RAM estimÃ©e |
|--------|-----------------|-------------|-------------|
| Dev | 1 | 2 | 512 MB |
| Petit | 2 | 5 | 1 GB |
| Moyen | 4 | 10 | 2 GB |
| Production | 4 | 20-50 | 4-8 GB |

### Lancer avec plus de workers

```bash
docker run -d --name llm-mq-poc \
  -p 8007:8007 \
  -e UVICORN_WORKERS=4 \
  -e LLM_WORKERS=20 \
  --env-file .env \
  llm-fastapi-mq
```

### Kubernetes (production)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-api
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: api
        image: llm-fastapi-mq:latest
        env:
        - name: UVICORN_WORKERS
          value: "4"
        - name: LLM_WORKERS
          value: "10"
        resources:
          limits:
            memory: "2Gi"
            cpu: "1000m"
```

---

## ğŸ–¥ï¸ Interface Web

### Lancer l'interface

```bash
# Windows
start chat.html

# macOS
open chat.html

# Linux
xdg-open chat.html

# Ou via serveur local
python -m http.server 3000
# Puis ouvrir http://localhost:3000/chat.html
```

### FonctionnalitÃ©s

- ğŸ’¬ Chat en temps rÃ©el avec streaming
- ğŸ”„ Switch entre mode RabbitMQ et Direct
- â±ï¸ Timestamps sur les messages
- ğŸ¯ Indicateur de typing pendant la gÃ©nÃ©ration
- ğŸ“Š Status indicators (API, Queue, Stream)
- ğŸ“± Responsive design

---

## ğŸ“ Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ main.py                 # Application FastAPI (routeur)
â”œâ”€â”€ config.py               # Configuration (env vars)
â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
â”œâ”€â”€ Dockerfile              # Image Docker multi-workers
â”œâ”€â”€ entrypoint.sh           # Lance API + Workers automatiquement
â”œâ”€â”€ run.sh                  # Script de gestion
â”œâ”€â”€ chat.html               # Interface web
â”œâ”€â”€ pytest.ini              # Configuration pytest
â”œâ”€â”€ .env                    # Variables d'environnement
â”œâ”€â”€ .env.example            # Template env
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py         # Module init
â”‚   â”œâ”€â”€ connection_pool.py  # Pool de connexions RabbitMQ (singleton)
â”‚   â”œâ”€â”€ llm_worker.py       # Worker LLM indÃ©pendant (scalable)
â”‚   â”œâ”€â”€ rabbit_publisher.py # Publisher RabbitMQ
â”‚   â””â”€â”€ rabbit_consumer.py  # Consumer RabbitMQ
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_concurrent.py  # Tests de parallÃ©lisme
```

---

## ğŸ”§ Scripts

```bash
./run.sh start    # Build + Run
./run.sh stop     # Stop container
./run.sh restart  # Restart
./run.sh logs     # Voir les logs
./run.sh shell    # Shell dans le container
./run.sh test     # Test les endpoints
```

---

## ğŸ› Troubleshooting

### Les requÃªtes sont traitÃ©es sÃ©quentiellement

**Cause** : Pas assez de workers LLM.

```bash
# VÃ©rifier le nombre de workers
docker top llm-mq-poc | grep llm_worker

# Augmenter les workers
docker run -e LLM_WORKERS=10 ...
```

### Connection error OpenAI

**Cause** : CaractÃ¨res `\r` dans le fichier `.env` (Windows).

```bash
# Nettoyer le fichier
sed -i 's/\r$//' .env
```

### RabbitMQ timeout / Connexion refusÃ©e

**Cause** : Limite du plan CloudAMQP gratuit (20 connexions max).

```bash
# RÃ©duire le nombre de workers
docker run -e LLM_WORKERS=3 -e UVICORN_WORKERS=2 ...
```

### Voir les stats de la queue

```bash
curl http://localhost:8007/stats
# {"pending_tasks":5,"queue":"llm_tasks","status":"ok"}
```

---

## ğŸ“„ License

MIT

---

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing`)
5. Ouvrir une Pull Request
