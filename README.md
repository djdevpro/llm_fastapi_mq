# ğŸš€ LLM Stream API

> **API LLM scalable** avec Celery pour gÃ©rer les requÃªtes OpenAI en parallÃ¨le.

[![Python 3.12](https://img.shields.io/badge/python-3.12-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.124-green.svg)](https://fastapi.tiangolo.com)
[![Celery](https://img.shields.io/badge/Celery-5.4-green.svg)](https://docs.celeryq.dev)
[![Docker](https://img.shields.io/badge/Docker-ready-blue.svg)](https://docker.com)

---

## ğŸ¯ ProblÃ¨me rÃ©solu

Les appels LLM (OpenAI, etc.) prennent **10-60 secondes** et bloquent vos workers HTTP.

**Cette architecture** :
- âš¡ Retourne **immÃ©diatement** (~100ms)
- ğŸ”„ Traite les requÃªtes **en parallÃ¨le** via Celery
- ğŸ“¡ Streame la rÃ©ponse via **Server-Sent Events**

---

## ğŸ“ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    POST /chat/async    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚    FastAPI      â”‚
â”‚             â”‚ â—„â”€â”€ task_id + session  â”‚      API        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚ SSE                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                               â”‚     Celery      â”‚
       â”‚                               â”‚    Workers      â”‚
       â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                        â”‚
       â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                               â”‚     Broker      â”‚
       â”‚ GET /stream/{session}         â”‚ Redis / RabbitMQâ”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Brokers supportÃ©s

| Broker | Config | Use case |
|--------|--------|----------|
| **Redis** | `BROKER=redis` | Simple, rapide (dÃ©faut) |
| **RabbitMQ** | `BROKER=rabbitmq` | CloudAMQP, haute dispo |

---

## ğŸ“ Structure du projet

```
llm_fastapi_mq/
â”œâ”€â”€ app/                          # Code source
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py               # FastAPI application
â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â””â”€â”€ llm_tasks.py          # TÃ¢ches Celery
â”‚   â”œâ”€â”€ celery_app.py             # Configuration Celery
â”‚   â””â”€â”€ config.py                 # Variables d'environnement
â”‚
â”œâ”€â”€ docker/                       # Docker
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.worker
â”‚   â”œâ”€â”€ entrypoint-api.sh
â”‚   â”œâ”€â”€ entrypoint-worker.sh
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ tests/
â”œâ”€â”€ chat.html                     # Interface web
â”œâ”€â”€ run.sh                        # Script de gestion
â””â”€â”€ requirements.txt
```

---

## âš™ï¸ Configuration

### Fichier `.env`

```env
# === REQUIS ===
OPENAI_API_KEY=sk-proj-xxxxx

# === BROKER ===
BROKER=redis
REDIS_URL=redis://redis:6379/0

# Pour RabbitMQ (optionnel)
# BROKER=rabbitmq
# RABBITMQ_URL=amqps://user:pass@host/vhost

# === API ===
PORT=8007
UVICORN_WORKERS=4

# === CELERY ===
CELERY_CONCURRENCY=4
CELERY_QUEUES=high,default,low
CELERY_LOGLEVEL=info

# === MONITORING ===
FLOWER_PORT=5555

# === RATE LIMITING ===
LLM_RPM=500
LLM_TPM=100000
```

---

## ğŸš€ DÃ©marrage rapide

### 1. Configuration

```bash
cp .env.example .env
# Ã‰diter .env avec votre clÃ© OpenAI
```

### 2. Lancer

```bash
./run.sh start
```

### 3. VÃ©rifier

```bash
curl http://localhost:8007/health/full
```

---

## ğŸ“¡ API Endpoints

| MÃ©thode | Endpoint | Description |
|---------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/health/full` | Status complet |
| `POST` | `/chat` | Chat sync (streaming HTTP) |
| `POST` | `/chat/async` | **Chat async (Celery)** âš¡ |
| `GET` | `/chat/{task_id}` | Status tÃ¢che |
| `GET` | `/stream/{session_id}` | Stream SSE |
| `POST` | `/embeddings` | Batch embeddings |
| `GET` | `/stats` | Stats queues |

### Exemple

```bash
# 1. Envoie (retour immÃ©diat ~100ms)
curl -X POST http://localhost:8007/chat/async \
  -H "Content-Type: application/json" \
  -d '{"message": "Hello!", "priority": 5}'

# RÃ©ponse:
# {"status":"queued","task_id":"xxx","session_id":"yyy","stream_url":"/stream/yyy"}

# 2. Stream SSE
curl -N http://localhost:8007/stream/yyy
# data: {"type":"chunk","content":"Hello"}
# data: {"type":"chunk","content":"!"}
# data: {"type":"complete"}
```

---

## ğŸ³ Commandes

```bash
./run.sh start         # DÃ©marre tout
./run.sh stop          # ArrÃªte
./run.sh restart       # RedÃ©marre
./run.sh logs          # Tous les logs
./run.sh logs api      # Logs API
./run.sh logs worker   # Logs Worker
./run.sh status        # Status
./run.sh scale 5       # 5 workers
./run.sh monitoring    # + Flower
./run.sh test          # Test endpoints
./run.sh build         # Build images
./run.sh clean         # Nettoie tout
```

---

## ğŸ“Š Scaling

### Configurations

| Charge | Workers | Concurrency |
|--------|---------|-------------|
| Dev | 1 | 2 |
| Petit | 2 | 4 |
| Moyen | 4 | 4 |
| Prod | 8+ | 4 |

### PrioritÃ©s

```python
{"priority": 10}   # â†’ queue "high"
{"priority": 0}    # â†’ queue "default"  
{"priority": -10}  # â†’ queue "low"
```

---

## ğŸ–¥ï¸ Interface Web

```bash
open chat.html
```

---

## ğŸ§ª Tests

```bash
pytest tests/test_celery.py -v -s
```

---

## ğŸ”§ Features

| Feature | Description |
|---------|-------------|
| Rate limiting | Token bucket Redis |
| Retry auto | Backoff exponentiel |
| PrioritÃ©s | 3 queues |
| Timeout | 5 min max |
| Monitoring | Flower |

---

## ğŸ“„ License

MIT
